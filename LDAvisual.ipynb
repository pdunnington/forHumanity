{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import spacy\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: report, tesla, vehicl, crash, autopilot, autonom, result, cruis, oper, alleg\n",
      "Topic 2: system, alleg, algorithm, use, voic, data, driver, result, caus, ask\n",
      "Topic 3: tesla, use, model, report, alleg, autopilot, generat, voic, driver, result\n",
      "Topic 4: use, report, algorithm, user, generat, alleg, sexual, ad, involv, deploy\n",
      "Topic 5: content, report, student, alleg, use, featur, due, video, school, moder\n",
      "Topic 6: content, user, report, algorithm, facebook, alleg, googl, result, use, post\n",
      "Topic 7: use, report, data, alleg, risk, incid, tool, user, predict, person\n",
      "Topic 8: use, system, fals, imag, report, alleg, amazon, algorithm, generat, polic\n",
      "Topic 9: alleg, algorithm, use, system, user, recognit, facial, autom, face, compani\n",
      "Topic 10: alleg, report, use, violat, system, polit, tesla, video, driver, facial\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "incidents = pd.read_csv(\"/Users/patrickdunnington/Desktop/DS_Capstone/msds_capstone_2024/personal_notebooks/patrick_nb/incidents_clean.csv\")\n",
    "\n",
    "# Convert date column to datetime format\n",
    "incidents['date'] = pd.to_datetime(incidents['date'])\n",
    "\n",
    "# Clean dataset\n",
    "for i in range(1, 565):\n",
    "    incidents.loc[i, 'reportnumber'] = incidents.loc[i, 'reports'].count(\",\") + 1\n",
    "\n",
    "# Clean description column\n",
    "incidents['clean_description'] = incidents['description'].str.replace('ai', '').str.replace('AI', '')\n",
    "\n",
    "# Tokenize and remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "incidents['clean_description'] = incidents['clean_description'].apply(preprocess_text)\n",
    "\n",
    "# Create document-term matrix\n",
    "vectorizer = CountVectorizer()\n",
    "incident_DTM = vectorizer.fit_transform(incidents['clean_description'])\n",
    "\n",
    "# Perform LDA\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=321)\n",
    "incident_topics = lda.fit_transform(incident_DTM)\n",
    "\n",
    "# Print top terms for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_terms = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_terms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdunnington/anaconda3/envs/bayesml/lib/python3.11/site-packages/sklearn/manifold/_mds.py:298: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import spacy\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# Read the CSV file\n",
    "incidents = pd.read_csv(\"/Users/patrickdunnington/Desktop/DS_Capstone/msds_capstone_2024/personal_notebooks/patrick_nb/incidents_clean.csv\")\n",
    "\n",
    "# Convert date column to datetime format\n",
    "incidents['date'] = pd.to_datetime(incidents['date'])\n",
    "\n",
    "# Clean dataset\n",
    "for i in range(1, 565):\n",
    "    incidents.loc[i, 'reportnumber'] = incidents.loc[i, 'reports'].count(\",\") + 1\n",
    "\n",
    "# Clean description column\n",
    "incidents['clean_description'] = incidents['description'].str.replace('ai', '').str.replace('AI', '')\n",
    "\n",
    "def lemmatization(incidents, allowed_postags=[\"NOUN\",\"ADJ\",\"VERB\",\"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable = [\"parser\", \"ner\"])\n",
    "    descript_out = []\n",
    "    for incident in incidents:\n",
    "        doc = nlp(incident)\n",
    "        new_descript = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_descript.append(token.lemma_)\n",
    "        final = ' '.join(new_descript)\n",
    "        descript_out.append(final)      \n",
    "    return (descript_out) \n",
    "\n",
    "lemmatized_decript = lemmatization(incidents['clean_description'])\n",
    "\n",
    "def gen_words(incidents):\n",
    "    final = []\n",
    "    for incident in incidents:\n",
    "        new = gensim.utils.simple_preprocess(incident, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_decript)\n",
    "\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for incident in data_words:\n",
    "    new = id2word.doc2bow(incident)\n",
    "    corpus.append(new)\n",
    "\n",
    "# print(corpus[0][0:20])\n",
    "\n",
    "word = id2word[[0][:1][0]]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=20,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto')\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds', R=30)\n",
    "pyLDAvis.save_html(vis, '/Users/patrickdunnington/Desktop/DS_Capstone/msds_capstone_2024/personal_notebooks/patrick_nb/ldavisual.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
